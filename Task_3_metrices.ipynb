{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgAvrA/27o5205PNRvYsL/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christina-26/Marvel-tasks-level-2/blob/main/Task_3_metrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression Metrics**\n",
        "Regression models have continuous output. So, we need a metric based on calculating some sort of distance between predicted and ground truth.\n",
        "\n",
        "In order to evaluate Regression models, we must understand these metrics in detail:\n",
        "\n",
        "Mean Absolute Error (MAE),\n",
        "Mean Squared Error (MSE),\n",
        "Root Mean Squared Error (RMSE),\n",
        "R² (R-Squared).\n",
        "Metrics used to evaluate these models should be able to work on a set of continuous values (with infinite cardinality), and are therefore slightly different from classification metrics.\n",
        "\n",
        "# **Mean Squared Error (MSE)**\n",
        "Mean squared error is perhaps the most popular metric used for regression problems. It essentially finds the average of the squared difference between the target value and the value predicted by the regression model.\n",
        "\n",
        "![1_gD-irKa-MIrwJc8uRBzQfg.gif](data:image/gif;base64,R0lGODlhQwFhAIMAAP///wAAANzc3BAQEKqqqmZmZiIiIkRERHZ2drq6ujIyMoiIiFRUVMzMzO7u7piYmCH5BAEAAAAALAAAAABDAWEAgwAAAIiIiJiYmERERAAAABAQEO7u7qqqqlRUVNzc3CIiIjIyMmZmZnZ2dszMzLq6ugT/QE5a7cVZb979B0MRcBCFYKrmXILxhWN5pmv7xnOwWQjrQeiEQ2LReEQaGwKCoHJIJKVTatV6DRmchIUK+wWHxWMYFMAgOCgNctv9hlPZgARqoo3n9Xt+aA7omTDrIyw0hMOTYHIC+Dt8hIxMGpTgknCUzNTcjMFEc0jkFB0lxcCsY6AsXWXNDJ3oSWmdpb1BOCgSiKpgCqr9Bf4weAg4wSVSMLhoCm52tnBYuGU61mE4UWBc233u7qb2DhefAh83P9cpR19nh1Fvh4/feJevtwegv9dnz9/3H+/3T+CzgAMN/ip4UCGrhBocEIAYUeJEihUtQlS2UOOMhhkMTFzQ/0DkSJIlRzJggGDACYuYNr4E0THDAYkFMsJI8GBFzY0GGjAYMKAazHkEhopAE7FLjRIRtRk0IAvAAztEix4VwRKiyxgJeihY2OBmIwIPrGpYJKOORLM30LQ1qADsBKpcz0pAYzdmzbE0gG4Y5s0BNwsrKTyUeheAgQUKCkQsoGBBXxAIJA7IoYBwBQO+vCHYnCEthQSPI2K9kBTiXNEDCkQWK8EzhQevbd/G/ToxKwOmIbq4QSzDAMrNDBDv4Hr4Cb10exQIXSEaAjWLBST7mwHbBQMHTuxm9VBidSOpzD3AvIEm3MIBCMzGkLJqBqrsJTh4DBzDY7299dNyLyLWiP+IhpMDHHkAPA1u0aC3pwqjaakMAijNqAYLeHACy8izYC37KmAQmAEkgk+HBT6M5IFsEhhAQQ0SGLCwDGkL4KECXqQQIw0am/BG0XTMgAEOaelNohltUJGU4wpAjYMFmmzgvwsaAAXI1PAhIMYKUJmpRAoskzC+6FqhSqIxaUBASk6K60CA9Hj5b8b0IBpSELPQcFECmvIU4EgJ+OOMPS9paUAiLW1Ig52P+hLuDjUBSM8Y7mSRlL4sMwAlAw8rCABFZ3qIKM8YaoMnG9IUIAmoDxsd8dHYPiKATQlgHeDMmayUIBlz1jothwbetKABBBbI8Jo2hCXWAmM/SIkCrcz/DLYtPKFhhKZDVRAwFVkrQGPABExAhybIth1hARepxJKwj3wUA10m1CWA3Q6KGeFNJoDFawI0BsV2olo5YMm2UEcwoGCDD0bY4D5Us+QGU38ArtChaOKXikYlpoBiEKiql65LKahxghP85OyAa8ZtEKJqhrEwBCYuuohcMZ5tToSWK3izBw7RIHmKnBPVl5kPHrL1Agf+WIsCn+6AqGjuCn1PA3H7mowdXiHydISyrqTjY5EJiI6BrIeQpY4YTwgbxTrq3KBTCqxMjCZ5Q7BWg24L4+wKmPe2KIaXIZobho/YnqBQR9a9oLSx73Ogcccfh7xxmS8h4PB4Ey9A7a1B/8D3seoeGGpfDRaHSIPvLJASXyn4Zl0iGUZcrYa1NXiMMI1fEK91iCqmoPaMowbho8XxpqCHY3YbWQPVK3jNI5XVe/SZIsGWfXOjGw665yoeCvMMoT+YfWhMLHOhAXip1y7lPKfeoGoO/tY9VkKKHF6Ym1GvvAK03XAPk/1BIBoI3LYGFCRASlQJHG188KMzoSGBtOGd0hI2QYQVogfaC8H3LGCZoSAOTv+yAgeVdrkPjokqk5uA6u4FnkJVrFCe8o/pgIeB3tSvFiOq2QhQNRyg6WmGiohCs6wwIg7dbgK6OMOgBPBAGmqpTB+6oOkiQxmvVAxWfmLMtZzRQh0ggP9fLyyeBhPACJVcAYywECMZl9eINSZOZ1v6oQTK1KTFlMU7FJKAATHUNjolDmrRawYTumeD62hqPpZBnxxFlsMhcEk2TaPNIjEwAEaWKV6QFNlNSpOb5TXKJwt4jQIYcKYD+AZwt5mITdpQm/GkJl6m0SIHEIhCEdRBVg4ICkoQSEPrbS+XYiOhBYSHgV4qhgOw41deaNkhTArhSR5ggl1oQoZoYmCaFyCVMT/QAlxBMAb0y0Unj9IDNjFgkEkAXQXIGZ9znoGR2qQDAmAVvQG+IIpGUKUC5edDvYjSCmW6CU366aJ8wlM9ThjRtdrYAcu8cwYB2E3/JlAafn2EjkX/kGgeC1DR+00gAA7V5igBIC62tbMDhVroCwQQPc2QxjMwEtUJrfCtifrTUpQxQCwNWrwJPKZER1Mp9mzQGzaxqAJAQImQNrACLCAVJYQr3KGos1MOdKZw3QzARS1VUBsERWogVaeo2mAuC/SJqhw4gDbW8iDkhGAtUIXBiLR6ABsCsWtwDQMSwzcBB2CQqiJNodckYFKPnKCuHlBR6W7wEISClQqMhZRjzzrYJ/TxPmK9wD1rcADYzccGCBAAA/z6BdCKdrIEK9FjpCIArW7Qsy8YRgAQ2crTKiStylIsAOS5A4gExbe/BW5wF9AYU1JEp7W1h1LhKDTCljV+z5UI6yCRe4929qALCcAs06C73WVOFx3HwcDLxtha75YXBAcAJERS0l3ztndKUHVgc9073x05RL30xS8IvLKB5OXXvw5xDFzf9V8C8wIy8WITYwq8YAY3mBBi+yYCDuvgsyjuBQiQz4QpnN+HaHjD9O3wh0XM12KOWJsrBSEMQmxigyJRiCouMYutMkbZvIkHw8VxjnFsnxXLWDFw2WEMeuxjYw5TyDEmMlGueWQPJ3kf5lyDjqU8XB4j2ckbsSmTr3wXi86AKqPd8j5kCgNhnWCj2Q2zPpia5v+Slc3z1WsP31xeyFJyzvMtLZjnGwEAOw==)\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "y_j: ground-truth value\n",
        "\n",
        "\n",
        "y_hat: predicted value from the regression model\n",
        "\n",
        "\n",
        "N: number of datums"
      ],
      "metadata": {
        "id": "_bVdyJ16VM3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9DcwBvDUclZ",
        "outputId": "0f06856a-c1fe-4168-ba12-dafb66868b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.4116666666666668\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_true=[1,2,3,4,5,6]\n",
        "y_pred=[0.5,2,3.4,5,5.5,6.9]\n",
        "print(\"Mean Squared Error:\",mean_squared_error(y_true,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last line computes the mean squared error between y_true and y_pred using the mean_squared_error function from scikit-learn. The MSE is a common metric used to measure the average squared difference between the predicted and true values. It quantifies the accuracy of a predictive model."
      ],
      "metadata": {
        "id": "xGpHmEBhaDTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Root Mean Squared Error (RMSE)**\n",
        "Root Mean Squared Error corresponds to the square root of the average of the squared difference between the target value and the value predicted by the regression model. Basically, sqrt(MSE). Mathematically it can be represented as:![1_2WxU7C-XVLN1AiMJw24b_g.gif](data:image/gif;base64,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)\n",
        "\n",
        "It retains the differentiable property of MSE.\n",
        "It handles the penalization of smaller errors done by MSE by square rooting it.\n",
        "Error interpretation can be done smoothly, since the scale is now the same as the random variable.\n",
        "Since scale factors are essentially normalized, it’s less prone to struggle in the case of outliers."
      ],
      "metadata": {
        "id": "57VvIHcQWexT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Root Mean Squared Error\",np.sqrt(mean_squared_error(y_true,y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9a7rSx0WqO4",
        "outputId": "24cb78b3-e3ea-44ff-a982-636839c5860b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error 0.641612551830672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAE**\n",
        "\n",
        "Mean absolute error (or mean absolute deviation) is another metric which finds the average absolute distance between the predicted and target values. MAE is define as below:![download (5).jpeg](data:image/jpeg;base64,UklGRi4SAABXRUJQVlA4WAoAAAAIAAAAdwUA7AAAVlA4IE4RAADwmACdASp4Be0APm02mkkkIyKhIZTooIANiWlu/BmZjutQy/2A/0nbH/b/Dv8d+Xfu/9k/ZL1xP6DyuRGvkf2//X/2j0V7+/jtqF/lX9R/1m9tgE6uvVu8Na7z906F//Q88P14E3P9KA9CNHGLO9jXUoD0I0cYs72NdSgPQjRxizvY11KA9CNHGLO9jXUoD0I0cYs72NdSgPQjRxizvY11I+1w9iiLqjm0o/6atNdSgPQjRxizvY11KA9CNHGLO9jXUoD0I0cYs72NdSgPQjRxUwdMSCkicW0kftz86XTFnexrqUB6EaOMWd7GupQHoRo4xZ3sa6lAehGjjFnexrqUBAgose7vOsh5f1hpIYY0yP/B1zQZN1KA9CNHGLO9jXUoD0I0cYs72NdSgPQjRxizvY11KA9CNHGHAyKidKuKrz5gSQPCoBJsa6lAehGjjFnexrqUB6EaOMWd7GupQHoRo4xXk9lRDm/aMHVMfGvVEpOKYZhEbEZHp8KJA7nY5Q5TWWyBu1qPI6fWQOe2eJ/dLCt+G8rU0cYszwQ/rnI6mQHexrqUB6EaOMWd7GupQHoRo4xZ2cYWgCXljoOZRKZfY7jIkxpTCzYnB/2qvwNVyMeHHxe5HvJSZjAfKZETSM+9I9jXux/jcRMUa9xJ4CmxHJvgv8EfhoaUMXG109R+WDacJF2cTYwVH8V0i7C+R3FPpq011KA9CNHGLO9jXUoD0I0cVWmU8xBEfxyRRL8asGppfMFqn/mfhoe+nBDfioE9R01vmcyz+K8HcxTuu8C2FqIaMgPQxb6OyYr2NO7ou9E+95KQuxdYJDWKQ1JNPfCfxT6vvLN5k+JQtYndMTET5uyxumYRqLx/AcaTrGZMsIaKtfHv5whKS9hz8G6lAehGjjFnexrqUB6EaOMWd7GupHssQ8Ok6sZFgXDDO2+MnqVAo+HFJ4qLjVAnE/hBjO5JTcEyaqrVUsGsruxv7JCZ8fcJBCyWS+ArNDAJz9Ta9E2SBO9snDnJLrihESr8ET9f1bJvjS2IXTwKQE3wRMYI2E1sP6ChgMMmkdZsfJjGun4rv4hcKPchEiBTNpnbNlnh0JkjjFnexrqUB6EaOMWd7GupQHoRo4rYXYo/2a5X3PY1Vn0/NL1usfW/2Du3ysAsocq1TwsN2I0yE/s5jo/XAUUwrR0RdwSeNPBvB439Dhh+cNYNSo+jscC14oF5VLOPMPoFwaf+Inx6+QAoYAxKIwes47XUoD0I0cYs72NdSgPQjRxizvY11KA9CNHGLO8q5hIVPfcN9wp5C5dRSM8jrquM9TBZ+7/6n/aO8Uqwtsv00I/1DM92sTs9dSgPQjRxizvY11KA9CNHGLO9jXUoD0I0cYs72NVoG0ze0mQDcQD3iY9WT62KVYt4snRUgatNdSgPQjRxizvY11KA9CNHGLO9jXUoD0I0cYs72NdSgPQoc3iOdxvvos/ZXVMu3wVCNGdTib9BYSMWd7GupQHoRo4xZ3sa6lAehGjjFnexrqUB6EaOMWd7GupQHoRvGews+vrp9sWEuh4GEGlEOlG8DikjXp8lSgPQjRxizvY11KA9CNHGLO9jXUoD0I0cYs72NdSgPQjRxizvY1/hUxZ3sa6lAehGjjFnexrqUB5qAAD+/6tgAAAAM4AUyn0dZ4CrcVyaiGCsX2UlTSgvyIkary/bg1uX39Dlwjk6miRt/56Tub0/eTZAAAvK3U9MB0bpBqS4CWJXXe6LznTEvcjPe0+5n1J6L+ZEJuRBNnx3ZC5OxptnUEIFzgMX4GynD/kd1rR3ZX1DBqLOwBYHWv8bionaPwGvc0VoAAC1fpHin4/DU593ba/jKoWnd1v6+nTR5uueVGfDLJqX2JNnvPDBZY/rKPP4t+gqmPADUlXisiidFDh3DJBH56fCg7D4JS2Zq8KTyM6hchr1AB1X/CcM9r5kNXLCgj87PXN1k+sa0nZ6SGFwAAHywN3Q7zQ2p6eRviJ0Ah2yOg89bUwvaC48lybAAR7lg+Ze+t2t815tbq72AzDomCKrXBvZSs6f1J6BYQTxzNeabUQG32nRg2fGpGAiR2QbriI9B0yTtsuCyIcSjH9WtrDk123MjxpIf9HHQfyLNWdj+NyYBMwPljUcuBSbLMJ10+acfEqNdZlrgikUrEipT3clihVjZ0M4SrBWHLZpQEgg6gSNDyK69j10LY7c7SZV7IbYM3LAgRw+GFh4DYwsgiC0uVyz2yzxmZ+ehoPbvdyzueJ3OUSTpQgPpVr8Dtc7l1FUtotK679RcyDhoG/u0ozPdu5a3eUr1crSlrnAtWcMvGJhHL3WApPD7BJJEywvxZowKVEl8RtPx1Zp6exe+XnhgRzCUE9FH3lJm/0Ud5rhEZFoM04bq9Nc9GAGH1u07dJRYKVRsiNWId5JJG29cmfYWCleamOshWOAovaK9j1lGjf0tEafWDLIGsnc2bshjJyEgpQ5kPFG09C5/sMK4u9HHKIoD46YkfW/cYxY/dt8LFdT2/6XL3du6+qJcSm2s1pF2daWM2OmZGYYC1avu5Q9ThLdXoaPa3xPFdUWlbs4enjcB3DQ5INFqbV5KCYVgW921w+OGVa5kf8YEeKxZ47D6el1Frv6gsQyr+UBuwKnV28ERO9zsb8enCHUuUloMBXN425b/rIf0MVRWCcU43+K6RhNbu//Fw1VCFBNMEw/uHB+0f2ZdhqXNOqrv30xcP1A7Q83WR1pxSQ403xgCgNGfrHTZCJwZgyDYUTduc0Ir0V1rwteRZVzPlTiTBWbjHVa+Jx0cwFU1eJGnUM8qPoLkXc1K1i7pTKzU1bRg7G3Xi7nOMUE/oq7pSLswfCdUbsXdgafYK07DzlZqqHpm8qpEbz4bzjIWxkIJSSAgAB2RdRJbZiDgGKOuW2qHyupB86Lhpc5C8sgUiXGDoEnYRVbayZJ994SksLeDlOWGPfLmkMCzvAOzpN0OclTzF3Jtn3FrP9kASdIZ82GqGtPAXXWLY75faiiJG2fZkdLV3969F9/+4TUU9Hv89WyaGXRyDTJ8sd/dfRzFJ535mvYjCTQVrWHwOAD52RiMfqPRt0UlSrjCNrZNSK+TxdEHQVZPSfaxWSda/RQ/Q0lRbKktwGampzAfHGaS0ODxDnrCkuLG17NskHnN+N8VgPFo/TsOBSAOKpUGrBHMH0ec8xkOpYhlUXRNi6c35AfAxw4JmsRgI0e3DZGcT+tdaeeev3t2Jhu2Q0v+/soaEwDpLqarl8nmCSOS7B+t6D2xEBt4jJIByH18d70XZuzZnUwX0DTcO2Si191uhFUtJiAPAKP6AV8015lFx9GDlhKNin3/VlLMl2wUJxwjZerj0Wkr0XRfMNZIbBENYtuE9B2kcVFwoFwlKkwJEMULLvp4dITGdkL7aSVyal0E56Gue0mHfXdMn+9Rs51Jdk/WLIsVE5eJZEnArn72dXSD9QLtVbmQj5rAwB9wvy39kY4tkQIG029ayaVkXitE4KlMtL7wTKlRt47VXpxLV7Os8AlGdHBxx8J0YbwFi/sjpAk+xzMLJqRQE9qEu95DcAzgX/Xaz2kBqagx/Xa8uZAI9Y1dOn7i15UG3uSGY9p8Dj9JkFjgPGFSwrRaceR7aBnzCC+/2bsLl+bsKrvn95k+qMRVg9HLBBncJyoyAB6LUW6i30bIT+Y2QuTTOt5bP98jR55cJBa40wnk2mya130Mee0S8Mn8R/Kkuadj7Zz4I+ZYAAi0KDU5/ZgEq7OpIZAlvGH0c2bQosoRUrwEYxcHSFW20sd4IxPD33B/UYpc6zMEy6IzZO710V95SzdyO2Pa6W45nPlsJLLRIqcpoohcW4p/cdE2RFPq2qNeJA+GVkoODjm53PKbV4Pmvm/qkqjgBSkQnH0Awz0NMYOVONr7xmbGvPCAnliyMllKXCbNRYfkoDEFY/2J/T22OmIeOwobFXgQu3gExSqfSUfzJ8tXWwo8wFt8rl3Z17Iuc0FNfwRazpYcqSl66kHicWaAYNeUbLjqARChUNm8nE1gfjSkn6OdFzw6n5uhTEuLX/Xz9hJYC+p0OlBIRoSmA8l4S5QwNKsA28ZOp6V8qcN1ynLFr3pbSLyMYuybp7IGsoovgIk5n6L9FC54Y235V40MM+8/c8yT8ptZOaGSG1r8l8D67RaLTyqee8wTEmTlNLbCLorSnU5oyYn8poTRRB6BumC8U4ZDNWIRl2YeVrrV6ogfoYgj7lbRAj95rLs7Et9Sq0lyN5rUKB1naxhXoDfwWtaRr9fz/ejOJq4MrC8ctM3MWts3GpLh2BDVYaf0WK8w1N+8v7JkyelEQMfzG0BJVwa+yYQ5hkc/Ai5HtD9JeAEOmoLK+5q6YmTVXb2eFj/jJnamZphRb1HyN7nbNTnJXdwUZsKji3GGvwl4PPNzckjsGxg5Jgl2mZNuQFiw64bnHr509ajKPHSB1W9jO7c/OLNHAtWXjKPyJkuNle/do8rL1eAIacGbRcqmupxrODzPDelhj6YZYDtzgNaRKBSNaOBW3zuPi4BIzFZdyDmxLE+odNBV/rCYsvVmb1PDr1c3JZ2EMJbvU68nEIReiYrz/qvg1zGMp1q3zoXGJ1mZv/Kjz5UZIh9zq0lyo871iQmQY0iQYRaSg2WyC/39YSnGGPzVPT4vTRb3Sclpv2QrQnGPHlAAAMCR+z61lAADOn/NL+qW04Y0/ZSFAKnIupFHbQsCxZhGnGZanIP21ArmxCu1t6M+ct2qTPlzuJsX/zwjYwEBgR5r+SHmuQczLT/rvVUEzSh5A0M1dxRNaydWBXq9G18j+Gk9w4viBRR+WGLErsJp1qQPbqYghs7Y+Gqh/Xsy5kwWDoEHmZTLNLBBmMMGHychndU5zr8XTJNaNu+xanaNTzXguOzrhVlygmm1fnaPqS+hduE/Lu2ErRY53xQUIc+sY92xxNGInSBMUG8rhehI/FJnajElqubnEz8aUywch43rr8IqEHp7S8cCb5l6QA0nEIZJurAbzAUCV3hjqlQWhz/j+7RH2xv7BmKmEkraAPo29h4cARV8GCDQrfQe8G6eWGGc2Ov7mTJfVGM1AUnqf/0K5G3bf9p/C870UWMPBjjB5vTnGULb4GeK3lwJjKFHHriNVaTXci/kHv1IBcpGn7+dxhmu7aO4hS0kmCwFjH3Fv1TLEJfas0yVNlxh+maKO7SZ+Y5lCxwmo7d9a8rZjY9dg9n9bx66XHEEDeNwpO9VPc8+sBu8+ZSAADazGZzHpyT8Lh99VskhjSUqUJ+NP5q+FuWxRByf/vZBp2Hq0Crk5DHUBmpR5yktKKCkFq3EvfIHIg6P5vB6MH+314Fcnuns09W7ZZw0k599tjF+QtEzMIvd6sPARxcRtagJUVuApMTZJHAjAPzKrymvuiHhIqkA5cBORUQyWZIArCR21Ydnw2WATFXd1BAwoGaPYmP5L2RJA4mS8/1LVbn7baXYl3YwTzmer4JbN84au1s/PVEz7r728iQAAIASNvOE+2QqtPZfK2vm/1I9Hiu11uuEXPgs3oye1vegyWP4Qb9ZS4Vp4wCBA6O3VGStJm4DvbI6UhIH+vw07SmhVCJwlgMrxKgCk5oltcCfD+nHrU+7PkUR0xTn0zDZiT4/W2VNB1eSLTrOEldNGrrSkhLIcDpDadtbtWkAAEeko7GDsHc3NTHDVKq1thBi7dWNWouwDYR5yVJjUBHQ8v9uXd+Q/9f8uoGhuaO+5NkuWVqCV2Z65+6Z1NXi40l2K7/p3fV9XwER976zX/4shMnDLo5sJFxgACKYXR58YO5H8jwEnN3cpPTa5I0uc9h1Ifo0BxIdd/WEoXLUUzHQK4VPGb0xIoLCQfQ0/k1dpPhyyVl8XffHxT4q1d5sH8puPn4/fHnkiQMH74IcFoyAAAAAAAAAEVYSUa6AAAARXhpZgAASUkqAAgAAAAGABIBAwABAAAAAQAAABoBBQABAAAAVgAAABsBBQABAAAAXgAAACgBAwABAAAAAgAAABMCAwABAAAAAQAAAGmHBAABAAAAZgAAAAAAAABIAAAAAQAAAEgAAAABAAAABgAAkAcABAAAADAyMTABkQcABAAAAAECAwAAoAcABAAAADAxMDABoAMAAQAAAP//AAACoAQAAQAAAHgFAAADoAQAAQAAAO0AAAAAAAAA)"
      ],
      "metadata": {
        "id": "bkMf_1GaXDBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "print(\"Mean Absolute Error:\",mean_absolute_error(y_true,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew-HuKVUXPPX",
        "outputId": "4865787c-a3f4-4b33-bf9e-6e19d1341ba4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification metrics**\n",
        "Classification models have discrete output, so we need a metric that compares discrete classes in some form. Classification Metrics evaluate a models performance and tell you how good or bad the classification is, but each of them evaluates it in a different way.\n",
        "in order to evaluate Classification models, we’ll discuss these metrics in detail:\n",
        "\n",
        "# 1. Accuracy\n",
        "\n",
        "Classification accuracy is perhaps the simplest metric to use and implement and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100.\n",
        "\n",
        "Higher accuracy values indicate that the model is making more correct predictions, while lower values suggest that the model is making more errors in its predictions.\n",
        "\n",
        "Classification accuracy is a metric that measures the proportion of correctly classified data points out of the total number of data points.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OpXg75ipXSOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_true=[0, 1, 2, 2, 0, 1]\n",
        "y_pred=[0, 2, 2, 1, 0, 1]\n",
        "print(\"Classification Accuracy:\",accuracy_score(y_true,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2BijgBzXb0O",
        "outputId": "d0bebbbe-c8ac-413a-b374-e03a95997c71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Confusion Matrix\n",
        "Confusion Matrix is a tabular visualization of the ground-truth labels versus model predictions. Each row of the confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class. Confusion Matrix is not exactly a performance metric but sort of a basis on which other metrics evaluate the results.\n",
        "\n",
        "confusion_matrix(y_true, y_pred)This line computes the confusion matrix by comparing the true labels (y_true) to the predicted labels (y_pred) using the confusion_matrix function. It returns a 2D array representing the confusion matrix."
      ],
      "metadata": {
        "id": "X4INslmNZHlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_true=[0, 1, 2, 2, 0, 1]\n",
        "y_pred=[0, 2, 2, 1, 0, 1]\n",
        "print(confusion_matrix(y_true,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjOp3tRgZxvi",
        "outputId": "8cd365ee-79e7-4e2e-96f0-ed40d056c0cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 0 0]\n",
            " [0 1 1]\n",
            " [0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rows of the matrix represent the true classes, and the columns represent the predicted classes. Each element in the matrix represents the count of samples falling into a particular category based on their true and predicted labels."
      ],
      "metadata": {
        "id": "bUbeNipnaxkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Precision\n",
        "Precision is the ratio of true positives and total positives predicted.A precision score towards 1 will signify that your model didn’t miss any true positives, and is able to classify well between correct and incorrect labeling.A low precision score (<0.5) means your classifier has a high number of false positives which can be an outcome of imbalanced class or untuned model hyperparameters.\n",
        "\n",
        "Precision for a class is the number of true positives (i.e. the number of items correctly labelled as belonging to the positive class) divided by the total number of elements labelled as belonging to the positive class (i.e. the sum of true positives and false positives, which are items"
      ],
      "metadata": {
        "id": "jRLI2QGwbDop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "print(\"Precision Score:\", precision_score(y_true, y_pred, average = 'micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seUmBf0oa9vJ",
        "outputId": "b18a1000-15a9-4458-ae6d-bc47d555f874"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Recall/Sensitivity/Hit-Rate\n",
        "A Recall is essentially the ratio of true positives to all the positives in ground truth.Recall, also known as the hit rate or true positive rate (TPR), is a classification metric that measures the ability of a classification model to correctly identify all relevant instances within a class. In other words, it quantifies the model's ability to find all positive examples correctly. Recall is particularly important when dealing with imbalanced datasets or when the cost of missing positive cases is high.\n",
        "\n",
        "Mathematically, recall is calculated as:\n",
        "\n",
        "Recall (TPR)=True Positives (TP)/True Positives (TP)+False Negatives (FN)\n",
        "\n"
      ],
      "metadata": {
        "id": "N1MWOzZ_co6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "print(\"Recall Score:\", recall_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Jk75RFdNXF",
        "outputId": "99d94614-5af9-41d1-9e54-5ac235f2691a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall Score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. F1-score\n",
        "The F1-score metric uses a combination of precision and recall. In fact, the F1 score is the harmonic mean of the two. The formula of the two essentially is:![1_GWsqmmfiZrsyrgtKEkjxnw.gif](data:image/gif;base64,R0lGODlhBAFRAIMAAP///wAAALq6uhAQEMzMzCIiIu7u7tzc3HZ2dmZmZoiIiKqqqpiYmERERFRUVDIyMiH5BAEAAAAALAAAAAAEAVEAgwAAAFRUVERERDIyMu7u7rq6ugAAAMzMzKqqqtzc3CIiIhAQEGZmZpiYmIiIiHZ2dgT/QE5a7cVZb979B0MNGZRFaER1ZVv3hWN5flKAaAwlmXv/BwaFwUehUtANlUtm0/kiMC4Bw+N5xWa1wYbRglxsxWNyGcMw2CgJg4FghsflS+rAQmi/53t+f0Vw4Kk4MAjzO0RM9HAwCFB8hIRUMBCMtLyMY3TA5OzcIrTyFB1dUpAiRU2dCXBUdX1VeTiFpa3NcJi11dVFyJVo0NsVHi0IrfAdTrZMMK5oVYaGJFhQqLauNkAWLljo9v4GXziI1qJqO0ffJJeYVAieYG6rXKePW8i+Najfh2MzQMiAh9/AMTncaFBAUGEWNAk1CFgY0cmkZxeiSMQ4BE+aCg3mjcsY/9IHAnkVIIpECQSNIXgnU76UMcklAQQK1LkgkFPnTp46Ya7bWKjbOZAtDKJDmjTPT2gkDRQFkAMDgQBemF4908aCHWcMqFjNolTsWLJlzZ5Fm1btWrZt2VaQWYEZBkJgsbjFm1fvXr59+1LYeFMCgXkU6mJFfKSkh8Mdjqp9lxjWg0IgGnfombmnZF0DGlk2YHcXVdERSQNpo6bD5V0BvIYO6frrD0b/QJe2xRqjbhcNwAkVnIG3ruELi2M5Tiv5wOVOmrt6Xi/6kumoqpO7LgSJamHZoXn38SDApAWuk4E/D5vzC/RXuDtXv75Fe+cOkceXv4J+kwP2r+xfD0Dq/P97QkDJtkukPy0QzC+W8Qoxzw8FrxCPvAgbVI5ADDeUYEIOPwTAQxBV8KtEE5HSQDNuNNvgRL5ScTHGvzB4LK3IKJAxrxHlEHHH/Hr0MUANg9yjAG0wMDILIOcgYYFmmmwmMVY+mBK5IedgA6A1bCOyjCWZPIgCkm7sUskr5WCAKwrSLNPLM+NQIEoA4nyFmQAOAMSBOyVIwAEBcnrAgeAExQWqqAJ1wIYHFrWgAUGLmGBRORsIlAEtD/CTB0G9ciEBl0Lo808CEK2AUAYMpVRQ1SQtAE948AOADdw8scKBAUI5wJBNGOAVACRASkABLQGYCQUJuAFgEzYqeGBYrpT/1YeCAG4SFgArGBDAGAXeY2LXXn/ls1oJirUB2RsG8IIRNaTqKFpVAAGAAZYA+AcHAAJY4I0kJbh1zQmynQDbBFJgdwJ3EYBo4KjcvbeiBwY4ACAB8p2g3yvsxVffWSyWYBaAOz5JgFmQqCQANScwuc43BsgljQLecNICRt5IoBc9cigMJAEqolcAsHR+hmQKHhBHggUE4+iKlwGIudSDambgZkosIKkSoik42oKsaeEyRPVktaCaRBF4Z4CTAYO1NnwmwMMqAQgUgCs2iiKkMCfAhsumBsimwOwL3u77mbnleooWJIKZ+ReGDZZTgiowcGBeCozsOnKsfVm7YGvf/1RCc8YxeFzrltddfOFaHPBvgGcC+HSCrSvAg9s5rUjgjV4oMJYd2t9IWnGQWMea9iyA1zq4wXpnu/e6Q7wXoneAJ3OUnSc43HXjARhgUKO1H1wd1fsWpHujVeP49eWvX+L1vrVnWvuur7Y362bkh8UArghQwKrBvzjZAZD8pID/vBYrLTXDURMgBAEloKeAzWJ/vgpDzbDwwMn173+fEmAAQoEwOzgAZgCZHwjr9JRKQaUAZ0JADRrAHQeoUDUCEBTbEuWoYY0rhhNoIS5qeEKTJCoLPMRAClfIQheyjQGCGse0tISAab2DiR58hW/atB7iTTEx6rMiU/CWRabUxAd+6OMiECIAADs=)\n",
        "\n",
        "A low F1 score tells you (almost) nothing — it only tells you about performance at a threshold. Low recall means we didn’t try to do well on very much of the entire test set. Low precision means that, among the cases we identified as positive cases, we didn’t get many of them right."
      ],
      "metadata": {
        "id": "O6VT6jNcdQIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "y_true=[0, 1, 2, 2, 0, 1]\n",
        "y_pred=[0, 2, 2, 1, 0, 1]\n",
        "print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82MReYBgdhc8",
        "outputId": "62d09f81-5ab4-446a-a3db-f42ac57a5b46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}